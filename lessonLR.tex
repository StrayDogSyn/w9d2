% Created 2025-06-09 Mon 13:58
% Intended LaTeX compiler: pdflatex
\documentclass[aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[style=apa, backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{./refs/refs.bib}
\AtEveryBibitem{\clearfield{note}}
\usepackage{endnotes}
\let\footnote=\endnote
\usepackage{./jtc}
\usetheme{default}
\author{Evan Misshula}
\date{\today}
\title{Introduction to Linear Regression (with Math and Python)}
\hypersetup{
 pdfauthor={Evan Misshula},
 pdftitle={Introduction to Linear Regression (with Math and Python)},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.3 (Org mode 9.6.15)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Motivation \& Intuition}
\label{sec:orgdf630c0}
\begin{frame}[label={sec:orgdbcf035}]{Why Learn Linear Regression?}
\begin{itemize}
\item Linear regression is the most basic and interpretable predictive model.
\item It helps understand relationships between variables.
\item Applications: housing prices, student performance, stock trends.
\end{itemize}
\end{frame}

\section{Mathematical Foundations}
\label{sec:orga638481}
\begin{frame}[label={sec:org2b966fa}]{The Linear Model}
Let \(\hat{y} = w_0 + w_1 x_1 + \dots + w_p x_p\).
In vector form: \(\hat{y} = \mathbf{x}^\top \mathbf{w}\).
\end{frame}

\begin{frame}[label={sec:orga9727d4}]{Terms and Concepts}
\begin{itemize}
\item \(y\): dependent variable (response)
\item \(\mathbf{x}\): feature vector (independent variables)
\item \(\mathbf{w}\): coefficients (weights)
\item \(w_0\): intercept
\item \(\hat{y}\): predicted value
\item Loss Function
\end{itemize}
\begin{block}{Mean Squared Error (MSE)}
\(\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)
  Minimizing this loss gives the best linear fit.


\begin{itemize}
\item Analytical Solution
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:orgff5d143}]{Derivation}
We want to minimize the MSE:

\begin{equation}
L(\mathbf{w}) = \| \mathbf{y} - \mathbf{X} \mathbf{w} \|^2 = (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w})
\end{equation}

Take the gradient with respect to \(\mathbf{w}\):

\begin{equation}
\nabla_{\mathbf{w}} L = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \mathbf{w})
\end{equation}

Set the gradient to zero:

\begin{equation}
\mathbf{X}^\top (\mathbf{y} - \mathbf{X} \mathbf{w}) = 0
\end{equation}

\begin{equation}
\Rightarrow \mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}
\end{equation}

\begin{equation}
\Rightarrow \hat{\mathbf{w}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
\end{equation}

\end{frame}
\end{document}

\begin{frame}[label={sec:org968e4c4}]{Normal Equation}
\(\hat{\mathbf{w}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\)
\end{frame}

\section{Derived by setting gradient of MSE to zero.}
\label{sec:org35a8899}
\section{Requires matrix invertibility.}
\label{sec:org3dd986e}

\begin{itemize}
\item Python Implementation (Manual)
** NumPy Code Skeleton
$\backslash$#+begin$\backslash$\textsubscript{src} python
import numpy as np
X = np.array($\backslash$[$\backslash$[1, x1], $\backslash$[1, x2], \ldots{}, $\backslash$[1, xn]])  \# with intercept
y = np.array($\backslash$[y1, y2, \ldots{}, yn])
w$\backslash$\textsubscript{hat} = np.linalg.inv(X.T @ X) @ X.T @ y
$\backslash$#+end$\backslash$\textsubscript{src}

\item Python Implementation (Library)
** Scikit-Learn
$\backslash$#+begin$\backslash$\textsubscript{src} python
from sklearn.linear$\backslash$\textsubscript{model} import LinearRegression
model = LinearRegression()
model.fit(X, y)
print(model.coef$\backslash$_, model.intercept$\backslash$_)
$\backslash$#+end$\backslash$\textsubscript{src}

\item Visualization
** Fitted Line and Residuals
\end{itemize}

\section{Plot data and regression line.}
\label{sec:orga8eb0b8}
\section{Show residuals (errors) as vertical lines.}
\label{sec:orga1f7cde}

\begin{itemize}
\item Model Evaluation
** R\textsuperscript{2} Score
\(R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}\)
\end{itemize}

\section{Indicates percentage of variance explained by the model.}
\label{sec:org887ee55}

\begin{itemize}
\item Assumptions and Pitfalls
** What Can Go Wrong?
\end{itemize}

\section{Nonlinearity}
\label{sec:orge9c03a8}
\section{Heteroscedasticity}
\label{sec:org614f51e}
\section{Multicollinearity}
\label{sec:org1915cae}
\section{Outliers}
\label{sec:org707f924}
\section{Autocorrelation}
\label{sec:orgf2488ad}

\begin{itemize}
\item Summary
\end{itemize}

\section{Linear regression is interpretable and foundational.}
\label{sec:orgd4af61c}
\section{Understand the math: model, loss, and solution.}
\label{sec:orgc0bf11a}
\section{Evaluate carefully with visual and numeric diagnostics.}
\label{sec:orgdd815b6}
\end{document}
